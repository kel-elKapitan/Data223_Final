{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Academy/Business_20_2019-02-11.csv...\n",
      "Downloading Academy/Business_21_2019-03-18.csv...\n",
      "Downloading Academy/Business_22_2019-04-15.csv...\n",
      "Downloading Academy/Business_23_2019-05-20.csv...\n",
      "Downloading Academy/Business_24_2019-07-15.csv...\n",
      "Downloading Academy/Business_25_2019-07-29.csv...\n",
      "Downloading Academy/Business_26_2019-08-12.csv...\n",
      "Downloading Academy/Business_27_2019-09-16.csv...\n",
      "Downloading Academy/Business_28_2019-10-21.csv...\n",
      "Downloading Academy/Business_29_2019-11-18.csv...\n",
      "Downloading Academy/Business_30_2019-12-30.csv...\n",
      "Downloading Academy/Data_28_2019-02-18.csv...\n",
      "Downloading Academy/Data_29_2019-03-04.csv...\n",
      "Downloading Academy/Data_30_2019-04-08.csv...\n",
      "Downloading Academy/Data_31_2019-05-20.csv...\n",
      "Downloading Academy/Data_32_2019-07-22.csv...\n",
      "Downloading Academy/Data_33_2019-08-05.csv...\n",
      "Downloading Academy/Data_34_2019-08-19.csv...\n",
      "Downloading Academy/Data_35_2019-09-23.csv...\n",
      "Downloading Academy/Data_36_2019-10-28.csv...\n",
      "Downloading Academy/Data_37_2019-11-18.csv...\n",
      "Downloading Academy/Data_38_2019-12-16.csv...\n",
      "Downloading Academy/Data_39_2019-12-30.csv...\n",
      "Downloading Academy/Engineering_17_2019-02-18.csv...\n",
      "Downloading Academy/Engineering_18_2019-04-01.csv...\n",
      "Downloading Academy/Engineering_19_2019-04-29.csv...\n",
      "Downloading Academy/Engineering_20_2019-05-27.csv...\n",
      "Downloading Academy/Engineering_21_2019-07-15.csv...\n",
      "Downloading Academy/Engineering_22_2019-07-22.csv...\n",
      "Downloading Academy/Engineering_23_2019-08-12.csv...\n",
      "Downloading Academy/Engineering_24_2019-09-16.csv...\n",
      "Downloading Academy/Engineering_25_2019-09-23.csv...\n",
      "Downloading Academy/Engineering_26_2019-10-28.csv...\n",
      "Downloading Academy/Engineering_27_2019-11-25.csv...\n",
      "Downloading Academy/Engineering_28_2019-12-16.csv...\n",
      "Downloading Academy/Engineering_29_2019-12-30.csv...\n",
      "Download completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "# Create a session using your AWS credentials\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Specify the bucket and the folder in the bucket\n",
    "bucket_name = 'data-eng-223-final-project'\n",
    "folder = 'Academy/'\n",
    "\n",
    "# List all files in the specified S3 bucket and folder\n",
    "objects = s3.list_objects(Bucket=bucket_name, Prefix=folder)\n",
    "\n",
    "# Define your new directory path\n",
    "new_directory = './s3_downloads'\n",
    "\n",
    "# Create new directory if it doesn't exist\n",
    "if not os.path.exists(new_directory):\n",
    "    os.mkdir(new_directory)\n",
    "\n",
    "# Loop over all files and download them one by one\n",
    "for obj in objects['Contents']:\n",
    "    file_name = obj['Key']\n",
    "    print(f'Downloading {file_name}...')\n",
    "    \n",
    "    # Download the file to the new directory\n",
    "    s3.download_file(bucket_name, file_name, os.path.join(new_directory, os.path.basename(file_name)))\n",
    "\n",
    "print('Download completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the path to your csv file\n",
    "file_path = '/Users/ShehryarMughal/Documents/Sparta/Final-Project/s3_downloads/Data_39_2019-12-30.csv'\n",
    "\n",
    "# Parse cohort and date from the file name\n",
    "file_name = os.path.basename(file_path).split('.')[0]\n",
    "split_name = file_name.split('_')\n",
    "cohort = split_name[0] + '_' + split_name[1]  # Combine \"Data\" and \"34\"\n",
    "date = split_name[2]\n",
    "\n",
    "# Read csv file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Initialize an empty dataframe to store the reshaped data\n",
    "reshaped_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over the unique weeks present in the column names\n",
    "for week in range(1, 9):\n",
    "    # Select the relevant columns for the current week\n",
    "    week_df = df[['name', 'trainer',\n",
    "                  f'Analytic_W{week}', f'Independent_W{week}',\n",
    "                  f'Determined_W{week}', f'Professional_W{week}',\n",
    "                  f'Studious_W{week}', f'Imaginative_W{week}']].copy()\n",
    "\n",
    "    # Rename the columns\n",
    "    week_df.columns = ['name', 'trainer',\n",
    "                       'Analytic', 'Independent',\n",
    "                       'Determined', 'Professional',\n",
    "                       'Studious', 'Imaginative']\n",
    "    \n",
    "    # Add a 'week' column\n",
    "    week_df['week'] = week\n",
    "\n",
    "    # Append the current week dataframe to the reshaped dataframe\n",
    "    reshaped_df = pd.concat([reshaped_df, week_df], ignore_index=True)\n",
    "\n",
    "# Add the 'cohort' and 'date' columns\n",
    "reshaped_df['cohort'] = cohort\n",
    "reshaped_df['date'] = date\n",
    "\n",
    "# Remove rows with any missing values\n",
    "reshaped_df.dropna(inplace=True)\n",
    "\n",
    "# Save the reshaped dataframe to a new csv file\n",
    "reshaped_df.to_csv('/Users/ShehryarMughal/Documents/Sparta/Data223/Final-Project/Transfromed-Files/Data_39.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Define the path to your directory\n",
    "dir_path = '/Users/ShehryarMughal/Documents/Sparta/Data223/Final-Project/Transfromed-Files-Data'\n",
    "\n",
    "# Get list of all csv files in the directory\n",
    "csv_files = glob.glob(os.path.join(dir_path, \"*.csv\"))\n",
    "\n",
    "# Initialize a list to store all the dataframes\n",
    "dfs = []\n",
    "\n",
    "# Loop over all csv files\n",
    "for file_path in csv_files:\n",
    "    # Read csv file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Append the dataframe to the list\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all dataframes\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Save the combined dataframe to a new csv file\n",
    "combined_df.to_csv('combined_data_files.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the path to your csv file\n",
    "file_path = '/Users/ShehryarMughal/Documents/Sparta/Data223/Final-Project/s3_downloads/Engineering_29_2019-12-30.csv'\n",
    "\n",
    "# Parse cohort and date from the file name\n",
    "file_name = os.path.basename(file_path).split('.')[0]\n",
    "split_name = file_name.split('_')\n",
    "cohort = split_name[0] + '_' + split_name[1]  # Combine \"Data\" and \"34\"\n",
    "date = split_name[2]\n",
    "\n",
    "# Read csv file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Initialize an empty dataframe to store the reshaped data\n",
    "reshaped_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over the unique weeks present in the column names\n",
    "for week in range(1, 9):\n",
    "    # Select the relevant columns for the current week\n",
    "    week_df = df[['name', 'trainer',\n",
    "                  f'Analytic_W{week}', f'Independent_W{week}',\n",
    "                  f'Determined_W{week}', f'Professional_W{week}',\n",
    "                  f'Studious_W{week}', f'Imaginative_W{week}']].copy()\n",
    "\n",
    "    # Rename the columns\n",
    "    week_df.columns = ['name', 'trainer',\n",
    "                       'Analytic', 'Independent',\n",
    "                       'Determined', 'Professional',\n",
    "                       'Studious', 'Imaginative']\n",
    "    \n",
    "    # Add a 'week' column\n",
    "    week_df['week'] = week\n",
    "\n",
    "    # Append the current week dataframe to the reshaped dataframe\n",
    "    reshaped_df = pd.concat([reshaped_df, week_df], ignore_index=True)\n",
    "\n",
    "# Add the 'cohort' and 'date' columns\n",
    "reshaped_df['cohort'] = cohort\n",
    "reshaped_df['date'] = date\n",
    "\n",
    "# Remove rows with any missing values\n",
    "reshaped_df.dropna(inplace=True)\n",
    "\n",
    "# Save the reshaped dataframe to a new csv file\n",
    "reshaped_df.to_csv('/Users/ShehryarMughal/Documents/Sparta/Data223/Final-Project/Transformed-Files-Engineering/Engineering_29.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Define the path to your directory\n",
    "dir_path = '/Users/ShehryarMughal/Documents/Sparta/Data223/Final-Project/Transfromed-Files-Engineering'\n",
    "\n",
    "# Get list of all csv files in the directory\n",
    "csv_files = glob.glob(os.path.join(dir_path, \"*.csv\"))\n",
    "\n",
    "# Initialize a list to store all the dataframes\n",
    "dfs = []\n",
    "\n",
    "# Loop over all csv files\n",
    "for file_path in csv_files:\n",
    "    # Read csv file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Append the dataframe to the list\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all dataframes\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Save the combined dataframe to a new csv file\n",
    "combined_df.to_csv('combined_engineering_files.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting the file Peter combined for the all business csv files, I consolidated all three files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Define the path to your directory\n",
    "dir_path = '/Users/ShehryarMughal/Documents/Sparta/Data223/Final-Project/Combined-Transformed-Files'\n",
    "\n",
    "# Get list of all csv files in the directory\n",
    "csv_files = glob.glob(os.path.join(dir_path, \"*.csv\"))\n",
    "\n",
    "# Initialize a list to store all the dataframes\n",
    "dfs = []\n",
    "\n",
    "# Loop over all csv files\n",
    "for file_path in csv_files:\n",
    "    # Read csv file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Append the dataframe to the list\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all dataframes\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Save the combined dataframe to a new csv file\n",
    "combined_df.to_csv('all_academy_files_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
